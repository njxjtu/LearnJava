package LearnJava;

/* Big O notation is a fundamental concept in computer science used to 
describe the efficiency or complexity of an algorithm. It's a way of 
classifying algorithms by how their performance or resource usage 
(like time or space) scales with the size of the input data. Instead of 
measuring the exact time an algorithm takes, which can vary based on 
hardware, Big O focuses on the rate of growth of the number of operations.
 */
public class BigOConcept {

    public static void main(String args[]){
        // O(1) - Constant Time
        int[] numbers = {1, 2, 3, 4, 5};
        int thirdElement = numbers[2]; // This operation is always one step.
    }

    
}
